{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d8e0604-11da-4617-baed-3ea73fb367f4",
   "metadata": {},
   "source": [
    "# **Preprocessing and Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ad287d-9ec0-457e-a784-c738e131b9a0",
   "metadata": {},
   "source": [
    "#### *Imports and Read in Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "935ff38c-29d6-495f-93ee-206570641323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, classification_report, f1_score, precision_score \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "06744e24-2600-42bc-b40a-b491bce0f79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>selftext</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author_premium</th>\n",
       "      <th>is_video</th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>post_char_length</th>\n",
       "      <th>post_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Jay_Subabove</td>\n",
       "      <td>This batter is hitting .191 with 3HR, 12 RBI, ...</td>\n",
       "      <td>1655862592</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Am I crazy?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>263</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Stress_Factor</td>\n",
       "      <td>Not a Yankees fan, but modern day record looki...</td>\n",
       "      <td>1655856890</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>MLB Record (Wins)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit         author  \\\n",
       "0          0   Jay_Subabove   \n",
       "1          0  Stress_Factor   \n",
       "\n",
       "                                            selftext  created_utc  \\\n",
       "0  This batter is hitting .191 with 3HR, 12 RBI, ...   1655862592   \n",
       "1  Not a Yankees fan, but modern day record looki...   1655856890   \n",
       "\n",
       "  author_premium  is_video  score              title  upvote_ratio  \\\n",
       "0          False     False      1        Am I crazy?           1.0   \n",
       "1          False     False      1  MLB Record (Wins)           1.0   \n",
       "\n",
       "   num_comments  post_char_length  post_word_count  \n",
       "0             0               263               53  \n",
       "1             0                96               19  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit = pd.read_csv('../data/reddit_cleaned.csv')\n",
    "reddit.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf59609-c9e1-4d21-be07-fe083130d50b",
   "metadata": {},
   "source": [
    "------\n",
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a488dac-ecb7-4032-acec-4b3f2e5324ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a function to remove html which seemed to be scattered throughout based on EDA\n",
    "# this code was adapted from the breakfast hour NLP practice for week 5\n",
    "\n",
    "def remove_html(post):\n",
    "    '''function to remove html and lowercase all text'''\n",
    "    post = post.lower()\n",
    "    no_html = BeautifulSoup(post).text\n",
    "    \n",
    "    return no_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "355ac86f-5939-400b-b247-56590bd0c48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/petermurphy/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/petermurphy/opt/anaconda3/envs/dsi/lib/python3.9/site-packages/bs4/__init__.py:404: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>selftext</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author_premium</th>\n",
       "      <th>is_video</th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>post_char_length</th>\n",
       "      <th>post_word_count</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Jay_Subabove</td>\n",
       "      <td>This batter is hitting .191 with 3HR, 12 RBI, ...</td>\n",
       "      <td>1655862592</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Am I crazy?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>263</td>\n",
       "      <td>53</td>\n",
       "      <td>this batter is hitting .191 with 3hr, 12 rbi, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Stress_Factor</td>\n",
       "      <td>Not a Yankees fan, but modern day record looki...</td>\n",
       "      <td>1655856890</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>MLB Record (Wins)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>19</td>\n",
       "      <td>not a yankees fan, but modern day record looki...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit         author  \\\n",
       "0          0   Jay_Subabove   \n",
       "1          0  Stress_Factor   \n",
       "\n",
       "                                            selftext  created_utc  \\\n",
       "0  This batter is hitting .191 with 3HR, 12 RBI, ...   1655862592   \n",
       "1  Not a Yankees fan, but modern day record looki...   1655856890   \n",
       "\n",
       "  author_premium  is_video  score              title  upvote_ratio  \\\n",
       "0          False     False      1        Am I crazy?           1.0   \n",
       "1          False     False      1  MLB Record (Wins)           1.0   \n",
       "\n",
       "   num_comments  post_char_length  post_word_count  \\\n",
       "0             0               263               53   \n",
       "1             0                96               19   \n",
       "\n",
       "                                          clean_text  \n",
       "0  this batter is hitting .191 with 3hr, 12 rbi, ...  \n",
       "1  not a yankees fan, but modern day record looki...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit['clean_text'] = reddit['selftext'].apply(remove_html)\n",
    "reddit.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b8fdd10c-8016-47de-a91f-5fbc691bce27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating functions that stem and lemmatize text - to be used as hyperparameters\n",
    "# this code was adapted from the breakfast hour NLP practice for week 5\n",
    "# lemmatize first\n",
    "\n",
    "def lemmatize_post(post):\n",
    "    '''\n",
    "    Function splits the text data,\n",
    "    lemmatizes it, and rejoins\n",
    "    '''\n",
    "    post_split = post.split()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in post_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "54988e2b-d9c7-4589-9bea-93de5cd729cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now for stemming\n",
    "\n",
    "def stem_post(post):\n",
    "    '''Same framework applied as lemmatize'''\n",
    "    post_split = post.split()\n",
    "    p_stemmer = PorterStemmer()\n",
    "    \n",
    "    return \" \".join([p_stemmer.stem(word) for word in post_split])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca464214-9319-4065-8f3e-74636539b120",
   "metadata": {},
   "source": [
    "------\n",
    "## **Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2379632c-941b-4073-9073-fe23961a4245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting with simple models - just text column and default hyperparameters\n",
    "# will tune once there appears to be a pipeline that works best\n",
    "\n",
    "X = reddit['clean_text']\n",
    "y = reddit['subreddit']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdf6ab6-c4c4-49aa-bf33-03bd39a4e1d3",
   "metadata": {},
   "source": [
    "### **Defining the baseline accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c42ac7c-55f5-4c38-99d4-d8e51cebe66f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.510562\n",
       "1    0.489438\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4323902-db66-4e7a-a01c-65b538dc5590",
   "metadata": {},
   "source": [
    "*hoping to beat the 51% baseline accuracy* (and ideally hit > 80% as defined in problem statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25e5a2b4-4a80-41ff-9295-f8e091b5d846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1668,)\n",
      "(557,)\n"
     ]
    }
   ],
   "source": [
    "# splitting the training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f3db4b-beaf-44ac-a1a9-4a9eafff4dd6",
   "metadata": {},
   "source": [
    "#### 1) Count Vectorizer and Naive Bayes (defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e82a796a-f6e4-41e4-909f-1d4a7fc5120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the pipeline\n",
    "pipe_cnb = Pipeline([\n",
    "    ('cvec',CountVectorizer()),\n",
    "    ('nb',MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5694aa6f-1b2b-417f-ad13-87b7a8fd4931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cvec', CountVectorizer()), ('nb', MultinomialNB())])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_cnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6144eb3-0510-4855-a147-e34cd74da6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8435251798561151\n",
      "Test accuracy: 0.7468581687612208\n"
     ]
    }
   ],
   "source": [
    "print(f'Training accuracy: {pipe_cnb.score(X_train, y_train)}')\n",
    "print(f'Test accuracy: {pipe_cnb.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b1a19e7-8da3-417c-9ddc-ca857516d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heavily overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "521af173-40ec-489e-9b1a-79c93d97a0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.66      0.73       284\n",
      "           1       0.70      0.84      0.76       273\n",
      "\n",
      "    accuracy                           0.75       557\n",
      "   macro avg       0.76      0.75      0.75       557\n",
      "weighted avg       0.76      0.75      0.75       557\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = pipe_cnb.predict(X_test)\n",
    "\n",
    "# print the classification report after generating predictions\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdd698af-9f9b-4b9f-a501-af302378eb4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7638190954773868"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at F1 Score\n",
    "f1_score(y_test,preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc9949a-e24a-4e93-9796-065212f79746",
   "metadata": {},
   "source": [
    "#### 2) Tfidf Vectorizer and Naive Bayes (defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da473de3-ca45-4729-8ed0-84aaa1dc163f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8039568345323741\n",
      "Test accuracy: 0.7019748653500898\n"
     ]
    }
   ],
   "source": [
    "# repeat the same process to train additional models (again with default parameters)\n",
    "pipe_tnb = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe_tnb.fit(X_train, y_train)\n",
    "\n",
    "print(f'Training accuracy: {pipe_tnb.score(X_train, y_train)}')\n",
    "print(f'Test accuracy: {pipe_tnb.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2038f999-a541-4cd0-8fdc-6d0b53ff7e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slightly worse accuracy and heavily overfit like before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de2e0f2b-f4f0-496c-a8ce-1419098b0ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.74      0.72       284\n",
      "           1       0.71      0.66      0.69       273\n",
      "\n",
      "    accuracy                           0.70       557\n",
      "   macro avg       0.70      0.70      0.70       557\n",
      "weighted avg       0.70      0.70      0.70       557\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = pipe_tnb.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd4c229e-eef3-43eb-bca8-60a7627e3d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6856060606060606"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ed4562-8c8e-4c01-a6ef-acbe91ae2e03",
   "metadata": {},
   "source": [
    "#### 3) Count Vectorizer and Logistic Regression (defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57748e33-96ab-47d9-8c58-eb659c13d47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8729016786570744\n",
      "Test accuracy: 0.7378815080789947\n"
     ]
    }
   ],
   "source": [
    "# repeat process with logistic regression\n",
    "pipe_clog = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('log', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipe_clog.fit(X_train, y_train)\n",
    "\n",
    "print(f'Training accuracy: {pipe_clog.score(X_train, y_train)}')\n",
    "print(f'Test accuracy: {pipe_clog.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "773fdf8d-9ba2-4e25-9065-3d5a5b78531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still heavily overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "151f1570-c6f2-44dd-b09c-8d1a48336830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.7533783783783783\n",
      "Precision Score: 0.6990595611285266\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.66      0.72       284\n",
      "           1       0.70      0.82      0.75       273\n",
      "\n",
      "    accuracy                           0.74       557\n",
      "   macro avg       0.74      0.74      0.74       557\n",
      "weighted avg       0.75      0.74      0.74       557\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = pipe_clog.predict(X_test)\n",
    "\n",
    "print(f'F1 Score: {f1_score(y_test, preds)}')\n",
    "print(f'Precision Score: {precision_score(y_test, preds)}')\n",
    "print(' ')\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e1f1d2-0466-4232-bf2d-40d8a9805ffd",
   "metadata": {},
   "source": [
    "#### 4) Tfidf  Vectorizer and Logistic Regression (defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c10a310-d2d6-4ee4-9b87-50abbfc8e3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8009592326139089\n",
      "Test accuracy: 0.7163375224416517\n"
     ]
    }
   ],
   "source": [
    "# repeat process with logistic regression\n",
    "pipe_tlog = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('log', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipe_tlog.fit(X_train, y_train)\n",
    "\n",
    "print(f'Training accuracy: {pipe_tlog.score(X_train, y_train)}')\n",
    "print(f'Test accuracy: {pipe_tlog.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7d127705-1dfa-4816-bdda-ce0f56146418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.7084870848708487\n",
      "Precision Score: 0.7137546468401487\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.73      0.72       284\n",
      "           1       0.71      0.70      0.71       273\n",
      "\n",
      "    accuracy                           0.72       557\n",
      "   macro avg       0.72      0.72      0.72       557\n",
      "weighted avg       0.72      0.72      0.72       557\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# slightly less overfit but still less than ideal\n",
    "preds = pipe_tlog.predict(X_test)\n",
    "\n",
    "print(f'F1 Score: {f1_score(y_test, preds)}')\n",
    "print(f'Precision Score: {precision_score(y_test, preds)}')\n",
    "print(' ')\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ac034a-9383-470d-b4a3-b6bf7956eefb",
   "metadata": {},
   "source": [
    "#### 1a) Count Vectorizer and Naive Bayes (fine tune w GridSearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e2425275-2a99-4a07-8254-0150c647d527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.7404076738609112\n",
      "Best Parameters: {'cvec__max_df': 0.9, 'cvec__min_df': 1, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': None}\n"
     ]
    }
   ],
   "source": [
    "# build the pipeline\n",
    "params_cnb = {\n",
    "    'cvec__max_df': [0.9, 0.95],\n",
    "    'cvec__min_df': [1,3,5,7,9],\n",
    "    'cvec__ngram_range': [(1,1), (1, 2)],\n",
    "    'cvec__stop_words': [None, 'english'],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe_cnb, param_grid=params_cnb, cv =3)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# print out best score and best params\n",
    "print(f'Best score: {gs.best_score_}')\n",
    "print(f'Best Parameters: {gs.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3b856417-05dc-46f8-9fab-c2adc191c0b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7396768402154399"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_test, y_test)\n",
    "#also try stopwords, preprocessor (functions created earlier), tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779f2252-5b2d-4f39-8d71-5830ef2036e6",
   "metadata": {},
   "source": [
    "#### 1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aaf582bc-c159-4a34-9040-3c120c05ccf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.740400280520041\n",
      "Best Parameters: {'cvec__max_features': 5000, 'cvec__preprocessor': <function stem_post at 0x7fa25604c3a0>}\n"
     ]
    }
   ],
   "source": [
    "# pipe_cnb = Pipeline([\n",
    "#     ('cvec',CountVectorizer(max_df=.9, min_df=1, ngram_range=(1,2))),\n",
    "#     ('nb',MultinomialNB())])\n",
    "\n",
    "# params_cnb = {\n",
    "#     'cvec__max_features': [2_000, 3_000, 4_000, 5_000],\n",
    "#     'cvec__preprocessor': [None, lemmatize_post, stem_post] # incorporating functions created earlier\n",
    "# }\n",
    "\n",
    "# gs = GridSearchCV(pipe_cnb, params_cnb, cv =5)\n",
    "# gs.fit(X_train, y_train)\n",
    "\n",
    "# # print out best score and best params\n",
    "# print(f'Best score: {gs.best_score_}')\n",
    "# print(f'Best Parameters: {gs.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "28f3e673-9e85-469f-b991-ddbb172022a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No significant improvement from tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ab48a0-3448-4c15-9da8-0f7b8fe457e9",
   "metadata": {},
   "source": [
    "#### 2a) Tfidf Vectorizer and Naive Bayes (fine tune w GridSearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a96fb086-8beb-4bda-a366-47fb11e3615a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7434052757793764\n",
      "Best Parameters: {'tvec__max_df': 0.9, 'tvec__min_df': 3, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': 'english'}\n"
     ]
    }
   ],
   "source": [
    "# repeat the same process to train additional models (again with default parameters)\n",
    "pipe_tnb = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "grid_params = {\n",
    "    'tvec__max_df': [0.9, 0.95],\n",
    "    'tvec__min_df': [1,3,5,7,9],\n",
    "    'tvec__ngram_range': [(1,1), (1, 2)],\n",
    "    'tvec__stop_words': [None, 'english'],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe_tnb, param_grid = grid_params, cv = 3)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best Score: {gs.best_score_}')\n",
    "print(f'Best Parameters: {gs.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b00f4681-ce88-43e5-9c3b-db34c78306f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minor improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9377c2-0418-4910-97cd-d4eecc0ecc00",
   "metadata": {},
   "source": [
    "#### 3a) Count Vectorizer and Logistic Regression (fine tune w GridSearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f2d2c2dc-d85b-48e2-9be5-adf7bd335872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7493841146535758\n",
      "Best Parameters: {'cvec__max_df': 0.9, 'cvec__min_df': 1, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english'}\n"
     ]
    }
   ],
   "source": [
    "# repeat process with logistic regression\n",
    "pipe_clog = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('log', LogisticRegression())\n",
    "])\n",
    "\n",
    "clog_params = {\n",
    "    'cvec__max_df': [0.9, 0.95],\n",
    "    'cvec__min_df': [1,3,5,7,9],\n",
    "    'cvec__ngram_range': [(1,1), (1, 2)],\n",
    "    'cvec__stop_words': [None, 'english'],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe_clog,\n",
    "                 param_grid= clog_params,\n",
    "                 cv = 5)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best Score: {gs.best_score_}')\n",
    "print(f'Best Parameters: {gs.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b7ec98fb-fccf-4635-b5c9-2d6513c881b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7504488330341114"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d915f04b-8d57-48bc-8bba-3bc8d03982b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7481937026847206\n",
      "Best Parameters: {'cvec__max_features': 5000}\n"
     ]
    }
   ],
   "source": [
    "# try running again\n",
    "pipe_clog = Pipeline([\n",
    "    ('cvec', CountVectorizer(max_df=.9,min_df=1,ngram_range=(1,2), stop_words = 'english')), \n",
    "    ('log', LogisticRegression())\n",
    "])\n",
    "\n",
    "clog_params = {\n",
    "   'cvec__max_features': [2_000, 3_000, 4_000, 5_000],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe_clog,\n",
    "                 param_grid= clog_params,\n",
    "                 cv = 5)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best Score: {gs.best_score_}')\n",
    "print(f'Best Parameters: {gs.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "cbb81fb4-4aaf-40f7-b4a4-7756172dd0e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7450628366247756"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1782d88-bde5-4efc-92fe-e0126df9c817",
   "metadata": {},
   "source": [
    "#### 3b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bc7ec0b3-dd66-4874-b4ff-558676988f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#consider different stopwords to use \n",
    "# Print English stopwords.\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b8ca175c-e9e1-4c71-b59c-d54cf5ad2f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "82c85f86-b706-45df-b299-734709eed195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'removed', 'poll', 'https', 'com', 'www', 'reddit', 'removed', 'poll', 'https', 'com', 'www', 'reddit']\n"
     ]
    }
   ],
   "source": [
    "new_words = ['removed', 'poll','https','com','www','reddit']\n",
    "for i in new_words:\n",
    "    stopword_list.append(i)\n",
    "\n",
    "print(stopword_list)\n",
    "stopword_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3ad88c31-429d-429f-999e-4604d31b4f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7451889014763265\n"
     ]
    }
   ],
   "source": [
    "# try running again\n",
    "pipe_clog = Pipeline([\n",
    "    ('cvec', CountVectorizer(max_df=.9,min_df=1,ngram_range=(1,2))), \n",
    "    ('log', LogisticRegression())\n",
    "])\n",
    "\n",
    "clog_params = {\n",
    "   'cvec__max_features': [2_000, 3_000, 4_000, 5_000],\n",
    "    'cvec__stop_words': [None, stopword_list]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe_clog,\n",
    "                 param_grid= clog_params,\n",
    "                 cv = 5)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best Score: {gs.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5f0a8562-6dcb-48f6-9ed4-bd78a00d4172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# still not getting close to the 80% threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e16b4dc-3fa3-4e63-8bb9-f303ab470512",
   "metadata": {},
   "source": [
    "#### 4a) Tfidf  Vectorizer and Logistic Regression (fine tune w GridSearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "68826b1e-5d8c-4147-992c-c5096eaa2d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7398081534772182\n",
      "Best Parameters: {'tvec__max_df': 0.9, 'tvec__min_df': 3, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': 'english'}\n"
     ]
    }
   ],
   "source": [
    "pipe_tlog = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('log', LogisticRegression())\n",
    "])\n",
    "\n",
    "tlog_params = {\n",
    "    'tvec__max_df': [0.9, 0.95],\n",
    "    'tvec__min_df': [1,3,5,7,9],\n",
    "    'tvec__ngram_range': [(1,1), (1, 2)],\n",
    "    'tvec__stop_words': [None, 'english'],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe_tlog,\n",
    "                 param_grid=tlog_params,\n",
    "                 cv = 3)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best Score: {gs.best_score_}')\n",
    "print(f'Best Parameters: {gs.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d6bfea72-5c4c-45e1-bdce-625876719857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# still not getting close to the 80% threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f998ce49-5338-4319-913c-cff32cae83fa",
   "metadata": {},
   "source": [
    "------\n",
    "## Changing Gears to Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "38f0cae2-ff0a-4c66-ac16-c2c605de3c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7110311750599521\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.9,\n",
       " 'cvec__min_df': 9,\n",
       " 'rf__max_depth': 5,\n",
       " 'rf__n_estimators': 200}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first attempt with CountVectorizer\n",
    "pipe_cvrf = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('rf', RandomForestClassifier(n_estimators=100))\n",
    "]) \n",
    "\n",
    "rf_params = {\n",
    "    'cvec__max_df': [0.9, 0.95],\n",
    "    'cvec__min_df': [1,3,5,7,9],\n",
    "    'rf__n_estimators': [100, 150, 200, 250],\n",
    "    'rf__max_depth': [1, 2, 3, 4, 5]}\n",
    "\n",
    "gs = GridSearchCV(pipe_cvrf, rf_params, cv = 3)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ea4fc64b-6dc0-4896-a199-4b51cfa05936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "30998366-fee6-47a1-b251-3eb4a985b9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.723021582733813\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english'}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_cvrf = Pipeline([\n",
    "    ('cvec', CountVectorizer(max_df = 0.9, min_df = 9)),\n",
    "    ('rf', RandomForestClassifier(n_estimators=200, max_depth=5))\n",
    "]) \n",
    "\n",
    "rf_params = {\n",
    "    'cvec__ngram_range': [(1,1), (1, 2)],\n",
    "    'cvec__stop_words': [None, 'english']\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe_cvrf, rf_params, cv = 3)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "77e9daa6-f552-4669-a5f9-7e4bf9e8b391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7062350119904077\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rf__max_depth': 5,\n",
       " 'rf__n_estimators': 150,\n",
       " 'tvec__max_df': 0.95,\n",
       " 'tvec__min_df': 5}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try the same setup with tfidf\n",
    "pipe_tvrf = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('rf', RandomForestClassifier(n_estimators=100))\n",
    "]) \n",
    "\n",
    "rf_params = {\n",
    "    'tvec__max_df': [0.9, 0.95],\n",
    "    'tvec__min_df': [1,3,5,7,9],\n",
    "    'rf__n_estimators': [100, 150, 200, 250],\n",
    "    'rf__max_depth': [1, 2, 3, 4, 5]}\n",
    "\n",
    "gs = GridSearchCV(pipe_tvrf, rf_params, cv = 3)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b3846025-eac8-446f-af7e-595e2d41f412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7134292565947242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.9,\n",
       " 'cvec__min_df': 9,\n",
       " 'et__max_depth': 5,\n",
       " 'et__n_estimators': 150}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attempt with ExtremelyRandomizeTrees\n",
    "# first attempt with CountVectorizer\n",
    "pipe_cvet = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('et', ExtraTreesClassifier(n_estimators=100))\n",
    "]) \n",
    "\n",
    "et_params = {\n",
    "    'cvec__max_df': [0.9, 0.95],\n",
    "    'cvec__min_df': [1,3,5,7,9],\n",
    "    'et__n_estimators': [100, 150, 200, 250],\n",
    "    'et__max_depth': [1, 2, 3, 4, 5]}\n",
    "\n",
    "gs = GridSearchCV(pipe_cvet, et_params, cv = 3)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9eb1ad-b216-40c6-b5d1-1f8329483d42",
   "metadata": {
    "tags": []
   },
   "source": [
    "------\n",
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ff69ff-5e4e-499f-b5a1-c3ac9226bebf",
   "metadata": {},
   "source": [
    "*AdaBoostClassifier*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "10d7c8ea-a8f0-4777-b62e-5ef8d7b71fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7260191846522782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ada__base_estimator__max_depth': 1,\n",
       " 'ada__learning_rate': 1.0,\n",
       " 'ada__n_estimators': 100,\n",
       " 'cvec__max_df': 0.95,\n",
       " 'cvec__ngram_range': (1, 2),\n",
       " 'cvec__stop_words': None}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try with CountVectorizer\n",
    "ada_cpipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('ada', AdaBoostClassifier(base_estimator=DecisionTreeClassifier()))\n",
    "])\n",
    "\n",
    "ada_params = {\n",
    "    'cvec__max_df': [0.9, 0.95],\n",
    "    'cvec__ngram_range': [(1,1), (1, 2)],\n",
    "    'cvec__stop_words': [None, 'english'],\n",
    "    'ada__n_estimators': [50, 100],\n",
    "    'ada__base_estimator__max_depth': [1, 2],\n",
    "    'ada__learning_rate': [.9, 1.0],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(ada_cpipe, param_grid = ada_params, cv = 3)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "cbde6a76-4197-4aa3-b2da-f24640256e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7182254196642686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ada__base_estimator__max_depth': 1,\n",
       " 'ada__learning_rate': 1.0,\n",
       " 'ada__n_estimators': 50,\n",
       " 'tvec__max_df': 0.9,\n",
       " 'tvec__ngram_range': (1, 1),\n",
       " 'tvec__stop_words': 'english'}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try with Tfidf Vectorizer\n",
    "ada_tpipe = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('ada', AdaBoostClassifier(base_estimator=DecisionTreeClassifier()))\n",
    "])\n",
    "\n",
    "ada_params = {\n",
    "    'tvec__max_df': [0.9, 0.95],\n",
    "    'tvec__ngram_range': [(1,1), (1, 2)],\n",
    "    'tvec__stop_words': [None, 'english'],\n",
    "    'ada__n_estimators': [50, 100],\n",
    "    'ada__base_estimator__max_depth': [1, 2],\n",
    "    'ada__learning_rate': [.9, 1.0],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(ada_tpipe, param_grid = ada_params, cv = 3)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f022f226-fd2f-4ca9-ad11-784acaf64fbe",
   "metadata": {},
   "source": [
    "*GradientBoostClassifier*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "84a3b064-05f7-4684-b470-07c975767c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7398081534772182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.9,\n",
       " 'cvec__ngram_range': (1, 2),\n",
       " 'cvec__stop_words': 'english',\n",
       " 'g__learning_rate': 0.9,\n",
       " 'g__n_estimators': 50}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try with CountVectorizer\n",
    "g_cpipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('g', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "ada_params = {\n",
    "    'cvec__max_df': [0.9, 0.95],\n",
    "    'cvec__ngram_range': [(1,1), (1, 2)],\n",
    "    'cvec__stop_words': [None, 'english'],\n",
    "    'g__n_estimators': [50, 100],\n",
    "    'g__learning_rate': [.9, 1.0],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(g_cpipe, param_grid = ada_params, cv = 3)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "cdadc2d7-45a7-4191-9ef5-9266576009b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7122302158273381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'g__learning_rate': 1.0,\n",
       " 'g__n_estimators': 100,\n",
       " 'tvec__max_df': 0.9,\n",
       " 'tvec__ngram_range': (1, 1),\n",
       " 'tvec__stop_words': 'english'}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try with Tfidf Vectorizer\n",
    "g_tpipe = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('g', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "ada_params = {\n",
    "    'tvec__max_df': [0.9, 0.95],\n",
    "    'tvec__ngram_range': [(1,1), (1, 2)],\n",
    "    'tvec__stop_words': [None, 'english'],\n",
    "    'g__n_estimators': [50, 100],\n",
    "    'g__learning_rate': [.9, 1.0],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(g_tpipe, param_grid = ada_params, cv = 3)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78b00ff-40c8-4a3e-8b9d-66ca34e00ae0",
   "metadata": {},
   "source": [
    "*VotingClassifier*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfff562-af0e-469f-8036-a2e926636335",
   "metadata": {},
   "source": [
    "-------\n",
    "## Conclusions and next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34214ec-dbdb-463c-9731-264b23378530",
   "metadata": {},
   "source": [
    "We will plan to cast a wider net in this case by advertising on both the r/mlb and r/redsox, given we were not able to create a model that  accurately predict the correct subreddit at 80% (i.e. difficult to train a model that is able to discern between the two - so we'd rather cast a wider net across the .) Despite not hitting the target accuracy and F1 scores of 80%, ran a sentiment analysis on the two subreddits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "d36c84ff-e8de-4e7a-83ce-b9b2d457ee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat an instance of the Vader Sentiment Intensity Analyzer\n",
    "sent = SentimentIntensityAnalyzer()\n",
    "\n",
    "# slice the df to just include the specific subreddits\n",
    "redsox = reddit[reddit['subreddit'] == 1]\n",
    "mlb = reddit[reddit['subreddit'] == 0]\n",
    "\n",
    "# calculate the polarity score for each post and create a list of the compound scores\n",
    "compound_sox_list = [scores_list[num]['compound'] for num in range(0,len([sent.polarity_scores(post) for post in redsox['selftext']]))]\n",
    "compound_mlb_list = [scores_list[num]['compound'] for num in range(0,len([sent.polarity_scores(post) for post in mlb['selftext']]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b04b8e45-e05d-4249-b3f4-b5bef029fc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r/redsox sentiment: 0.19929403122130396\n",
      "r/mlb sentiment: 0.19729119718309857\n"
     ]
    }
   ],
   "source": [
    "# checking the sentiment across all posts in the self text column\n",
    "print(f'r/redsox sentiment: {np.mean(compound_sox_list)}')\n",
    "print(f'r/mlb sentiment: {np.mean(compound_mlb_list)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68173049-96d3-45b3-a2f3-cd9d3c6ce5e5",
   "metadata": {},
   "source": [
    "Both positive which is good to know - may be more receptice to ads - and given how similar the sentiments are, makes the case to cast wider net by advertising "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi] *",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
